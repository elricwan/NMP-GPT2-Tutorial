# -*- coding: utf-8 -*-
"""story_hf_trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tUD4457MkSZZKUpsGwuA1-mGTySIJ6rV

## Introduction:


## Some infomation about this task:

About the task which we will show here is story generation.

1. Story generation: We will use the GPT-2 to train a model which can generate some stories.
2. Dataset: In huggingface "KATANABRAVE/stories"
3. [GPT model](https://huggingface.co/docs/transformers/v4.32.0/en/model_doc/gpt2#transformers.GPT2Model), we will use the model via huggingface.

Before run this notebook, please ensure that these packages you have already installed.

Python version: 3.8

Packages:
numpy pandas torch torchvision torch-optimizer tqdm accelerate transformers matplotlib datasets huggingface-hub sentencepiece argparse tensorboard

**If not**, please run these codes to install all the package whcih we need. And if you have more packages whcih you want to usem. Please add them in the requirements.txt. When you upload the project, please upload the requirements.txt which you modified.

> Install Netmind-Mixin-Runtime:
```bash
!git clone https://github.com/protagolabs/NMP-GPT2-Tutorial.git
!pip install -r NMP-GPT2-Tutorial/requirements.txt
!pip install git+https://github.com/protagolabs/NetMind-Mixin-Runtime@py310
```
"""

# !git clone https://github.com/protagolabs/NMP-GPT2-Tutorial.git
# !pip install -r NMP-GPT2-Tutorial/requirements.txt
# !pip install git+https://github.com/protagolabs/NetMind-Mixin-Runtime@py310

"""## Not-Netmid-Part

### Step 1: Load the model and tokenizer
"""

from transformers import GPT2Tokenizer, AutoModelForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
model = AutoModelForCausalLM.from_pretrained("gpt2")

"""### Step 2: Prepare the dataset.

"""

from datasets import load_dataset
from transformers import DataCollator


# Import the dataset, which is a demo for some D&D stories.
dataset = load_dataset("KATANABRAVE/stories")

"""### Step 3: Define the TrainingArguments

"""

# HuggingFace Trainer
import transformers
from transformers import TrainingArguments

# Here we want to close the wandb, if we use the huggingface's tranier.
import os
os.system("wandb offline")

training_args = TrainingArguments(
        evaluation_strategy = "epoch",
        learning_rate=2e-4,
        save_steps = 1000,
        weight_decay=0.01,
        per_device_train_batch_size=4,
        do_eval = False,
        do_train = True,
        num_train_epochs = 1,
        output_dir = "./model"
    )

"""### Step 4: Define the optimizer"""

"""
Load optimizer.
"""
from torch_optimizer import Adafactor
from torch.nn.utils import clip_grad_norm_
from transformers import get_linear_schedule_with_warmup, AdamW
# setup optimizer...
model.train()

param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': training_args.weight_decay},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]
optimizer = AdamW(optimizer_grouped_parameters, lr=training_args.learning_rate)

"""### Step 5: Set the GPU"""

import torch

device = torch.device("cuda:{}".format(training_args.local_rank))
model = model.to(device)

"""## Netmid-Part

### Step 6: Initialize the Netmind nmp

The Enviroments' parameters mush be set before import nmp

- load_checkpoint (Default True): Whether to load checkpoint saved in previous training. It only works when training on the Netmind platform.
- use_ddp (Default False): Whether to use ddp in current training.  

Initialize the data of the "Mixin" library. This function should be placed before the actual start of training, and  before other "Mixin" API calls
"""

import os
os.environ["PLATFORM"] = os.getenv("PLATFORM", "pytorch")
# os.environ["MASTER_ADDR"] = os.getenv("MASTER_ADDR", "127.0.0.1")
# os.environ["MASTER_PORT"] = os.getenv("MASTER_PORT", "8000")
# os.environ["WORLD_SIZE"] = os.getenv("WORLD_SIZE", "1")
# os.environ["RANK"] = os.getenv("RANK", "0")

from NetmindMixins.Netmind import nmp, NetmindOptimizer, NetmindDistributedModel

nmp.init(load_checkpoint=False, use_ddp=False)

"""### Step 7: Define the NetmindTrainerCallback

We will use it in the trainer initialize
"""

import transformers
from NetmindMixins.Netmind import NetmindTrainerCallback

class CustomTrainerCallback(NetmindTrainerCallback):
    def __init__(self):
        super().__init__()

    '''
    Add custom training metrics
    '''

    def on_step_end(self, args: transformers.TrainingArguments, state: transformers.TrainerState,
                    control: transformers.TrainerControl, **kwargs):
        kwargs["custom_metrics"] = {}
        return super().on_step_end(args, state, control, **kwargs)

    '''
    Add custom evaluation metrics
    '''

    def on_evaluate(self, args: transformers.TrainingArguments, state: transformers.TrainerState,
                    control: transformers.TrainerControl, **kwargs):
        kwargs["custom_metrics"] = {}
        return super().on_evaluate(args, state, control, **kwargs)

"""### Step 8: Set the model to NetmindDistributedModel

- model: Model variable.
  
Wrap machine learning model with "NetmindDistributedModel". This will not change the ML model itself. It can be placed anywhere after the "model" is defined and before the actual start of training.
"""

from transformers import AdamW
import torch

ddp_model = NetmindDistributedModel(
    model)

"""### Step 9: Define the optimizer, and set it be the NetmindOptimizer

- optimizer: optimizer variable.

Wrap machine learning model optimizer with "NetmindOptimizer". This will not change the optimizer itself. It can be placed anywhere after the "optimizer" is defined and before the actual start of training.
"""

optimizer = NetmindOptimizer(optimizer)
schedule_total = training_args.max_steps

scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=training_args.warmup_steps, num_training_steps=schedule_total
)

optimizers = (optimizer, scheduler)

"""### Setp 10: Start Training"""

from transformers import Trainer

nmp.init_train_bar(max_steps=training_args.max_steps)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    optimizers=optimizers,
    callbacks=[CustomTrainerCallback],
)
trainer.remove_callback(transformers.trainer_callback.PrinterCallback)
trainer.train()

nmp.finish_training() # Finish the training. It should be placed at the end of file

"""### Step 10: Save Model

If you want to save the model, please add some function here to save the model which you want to get.
"""

